{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visit loss and Normalization\n",
    "Visit loss implicitely normalizes along classes, i.e. it assumes that the unsupervised samples have the same class distribution as the supervised samples. \n",
    "\n",
    "This can be a problem in the following cases:\n",
    "- Settings with few supervised samples, where we don't want to equalize count by sampling when creating batches, as in active learning. \n",
    "- Settings with many classes, where a batch cannot cover all samples.\n",
    "\n",
    "This problem is worsened by the fact that the labels of unsupervised samples are unknown, so one cannot sample equally distributed samples (in every batch) from the unsupervised training data.\n",
    "\n",
    "Examples for this normalization:\n",
    "\n",
    "#### unbalanced supervised samples (active learning)\n",
    "1 sup sample of class A, 2 sup samples of class B. 2 unsupervised samples.\n",
    "\n",
    "\\begin{equation*}\n",
    "P_{ab} =  \\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "-> visit probability is (0.33, 0.66), p_target is (0.5, 0.5) -> strange things might happen.\n",
    "\n",
    "\n",
    "#### unbalanced unsupervised samples\n",
    "1 sup sample of class A, 1 sup sample of class B. 4 unsupervised samples. By random (bad) luck, we got 1 unsupervised sample of A and 3 of B, so, if the model would be good we get the following P_ab:\n",
    "\n",
    "\\begin{equation*}\n",
    "P_{ab} =  \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & .33 & .33 & .33 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "-> visit probability is (0.5,  0.165,  0.165,  0.165), p_target is (0.25, 0.25, 0.25, 0.25) -> strange things might happen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple case\n",
    "P_ab = np.asarray([[1,0], [0,1],[0,1]])\n",
    "P_ba = np.asarray([[1,0,0],[0,1,0]])\n",
    "labels_raw = np.asarray([0,1,1], np.int)\n",
    "num_classes = 2\n",
    "\n",
    "#simple case with unbalanced unsup samples\n",
    "P_ab = np.asarray([[1,0,0,0], [0,0.33,0.33,0.33]])\n",
    "P_ba = np.asarray([[1,0],[0,1],[0,1],[0,1]])\n",
    "labels_raw = np.asarray([0,1], np.int)\n",
    "num_classes = 2\n",
    "\n",
    "# longer case\n",
    "#P_ab = np.asarray([[0.4,0.6,0,0], [0,0,0,1], [0.6,0.4,0,0],[0,0,1,0]])\n",
    "#labels_raw = np.asarray([0,1,0,1], np.int)\n",
    "#num_classes = 2\n",
    "\n",
    "# case where a class has no samples\n",
    "#P_ab = np.asarray([[0.4,0.6,0,0], [0,0,0,1], [0.6,0.4,0,0],[0,0,1,0]])\n",
    "#labels_raw = np.asarray([0,1,0,1], np.int)\n",
    "#num_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.5       ,  0.16500001,  0.16500001,  0.16500001]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# current implementation\n",
    "\n",
    "p = tf.placeholder(shape=[None, None], dtype=tf.float32)\n",
    "\n",
    "visit_probability = tf.reduce_mean(\n",
    "        p, [0], keep_dims=True, name='visit_prob') \n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "print(sess.run([visit_probability], {p: P_ab}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class-normalized visit loss\n",
    "\n",
    "Visit loss assumes an equal class distribution among supervised samples. If that is not true, this assumption can be removed by scaling P_ab with the class counts, and then using sum instead of mean to calculate visit probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 1.        ,  0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.33000001,  0.33000001,  0.33000001]], dtype=float32), array([[ 0.50251257,  0.16582915,  0.16582915,  0.16582915]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# class normalized\n",
    "\n",
    "# from semisup loss\n",
    "labels = tf.placeholder(shape=[None,], dtype=tf.float32)\n",
    "equality_matrix = tf.equal(tf.reshape(labels, [-1, 1]), labels)\n",
    "equality_matrix = tf.cast(equality_matrix, tf.float32)\n",
    "p_target = (equality_matrix / tf.reduce_sum(\n",
    "    equality_matrix, [1], keep_dims=True)) \n",
    "scale_f = tf.diag_part(p_target)\n",
    "\n",
    "p = tf.placeholder(shape=[None, None], dtype=tf.float32)\n",
    "p_norm = tf.transpose(tf.multiply(tf.transpose(p), scale_f))\n",
    "\n",
    "visit_probability = tf.reduce_sum(\n",
    "        p_norm, [0], keep_dims=True, name='visit_prob') \n",
    "\n",
    "# normalization\n",
    "visit_probability = visit_probability * (1 / tf.reduce_sum(visit_probability))\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "print(sess.run([p_norm, visit_probability], {labels: labels_raw, p: P_ab}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximity loss\n",
    "Visit loss assumes an equal class distribution among unsupervised samples. If that is not true (i.e. small unsup batch size and large number of classes), this assumption can be removed by using $P_{bab}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.25  ,  0.2475,  0.2475,  0.2475]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "p_ab = tf.placeholder(shape=[None, None], dtype=tf.float32)\n",
    "p_ba = tf.placeholder(shape=[None, None], dtype=tf.float32)\n",
    "\n",
    "p_bab = tf.matmul(p_ba, p_ab, name='p_bab')\n",
    "\n",
    "visit_probability = tf.reduce_mean(p_bab, [0], name='visit_prob_bab', keep_dims=True)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "print(sess.run([visit_probability], {p_ab: P_ab, p_ba: P_ba}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
